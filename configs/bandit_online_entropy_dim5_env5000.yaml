# Online training configuration for bandit environment
env: "bandit"
envs: 5000
hists: 1
samples: 1
H: 100
dim: 5
var: 0.3
cov: 0.0

# Model configuration
embd: 32
head: 4
layer: 4
lr: 0.001
dropout: 0
shuffle: true

# Online training configuration
online_training: true
n_epoch: 100
seed: 1

# Context length cap to prevent quadratic scaling (default: 1024)
# Set to 1024 for full context. Training will be slower but uses gradient checkpointing for efficiency.
max_context_length: 1024
print_every: 5

# Evaluation configuration
n_eval: 100
test_cov: -1.0

# ========================================
# Entropy penalty coefficient (beta) scheduling
# ========================================
# Options: 'linear', 'stepped', 'linear_interpolate', 'constant',
#          'self_referential_entropy', 'reward_variance', 'inverse_sqrt', 'reward_prediction_error'

# HEURISTIC SCHEDULES (pre-defined decay):
# ----------------------------------------

# For linear: Loss = CE - beta * Entropy, beta decreases from confidence_start to 0
# confidence_type: linear
# confidence_start: 0.1  # Starting beta value (default: 0.1)

# For stepped: Loss = CE - beta * Entropy, beta decreases from confidence_start to 0 at max_position, then stays at 0
# confidence_type: stepped
# confidence_start: 0.1
# max_position: 40  # Position where beta reaches 0 (default: 40)

# For linear_interpolate: Loss = (1-beta) * CE - beta * Entropy, beta decreases from confidence_start to 0
# confidence_type: linear_interpolate
# confidence_start: 0.1

# For constant: beta stays constant
confidence_type: constant
confidence_value: 0.0

# PRINCIPLED SCHEDULES (mathematically grounded, adaptive):
# ----------------------------------------------------------

# 1. Self-Referential Entropy (RECOMMENDED - Most Elegant)
#    β_h = α · H[π_θ(·|s_h, D_{h-1})]
#    The policy's own uncertainty directly controls the regularization strength.
#    High entropy → more regularization → maintains exploration
#    Low entropy → less regularization → allows exploitation
#    Naturally adapts without heuristics.
# confidence_type: self_referential_entropy
# alpha: 0.05  # Scaling factor for policy entropy (default: 0.05, reduced from 0.1)

# 2. Reward Variance Schedule
#    β_h = α · min(σ²(r_{1:h-1}) / (μ(r_{1:h-1})² + ε), β_max)
#    Coefficient-of-variation style normalization with clipping.
#    High reward variance indicates epistemic uncertainty about the environment.
#    Information-theoretic and adapts to environment noisiness.
# confidence_type: reward_variance
# alpha: 1.0  # Scaling factor (default: 1.0)
# beta_max: 0.3  # Maximum beta to prevent explosion (default: 0.3)
# epsilon: 1e-6  # Small constant for numerical stability

# 3. Inverse Square Root (From Online Learning Theory)
#    β_h = β_0 / √(h+1)
#    Optimal regret bounds in online learning and bandit algorithms follow O(√T).
#    Not a heuristic - derived from theoretical analysis of exploration-exploitation tradeoffs.
# confidence_type: inverse_sqrt
# beta_0: 0.5  # Initial beta value (default: 0.5, reduced from 1.0 to avoid over-regularization)

# 4. Reward Prediction Error
#    β_h = α · min(|r_h - E[r_{1:h-1}]| / (std(r_{1:h-1}) + ε), β_max)
#    Statistically grounded in predictive uncertainty with clipping.
#    Large prediction errors indicate model uncertainty, warranting more regularization.
# confidence_type: reward_prediction_error
# alpha: 0.1  # Scaling factor for normalized prediction error (default: 0.1)
# beta_max: 0.3  # Maximum beta (default: 0.3)
# epsilon: 1e-6  # Small constant for numerical stability

